1.e.g.:
	f(censors of information) = angles of steeling wheel  (self-driving car)
 	f(userA, goodsB) = the probability of purchase (recommendation system)
	
2.reading about estimating the combat power(CP) of a pokemon after evolution:
	step 1 model  => y = b + w * xcp  ; b, w  parameters 
	                 y(pokemon after evolution) = f(pokemon x before evlution)
			 y = b + Σwixi (linear regression)  
			   => a set of function
	step 2 goodness of function
		L(f) = L (w, b) => loss function: function's function => output : how bad of the fuction
				 => evaluate a set to parameter b, w
				 = Σ real value - estimated value
				 => finding the minimun of the loss function => Gredient Decent
	step 3 Gredient Decent
		η : learning rate
		compute: dL/dW|w=w0   1w <- w0- ηdL/dW|w=w0  => many iterations
		Evaluate the model: the average of error of the training data &
																						 of testing data
  	step 4 optimizing:select another model
		y = b + w1 * xcp + w2 * xcp<SUP>2</SUP> + w3 * xcp<SUP>3</SUP> + w4 * xcp<SUP>4</SUP> +...
	 	=> Overfitting 
	 	A more complex model does not always lead to better performance on Testing Data
	 	=> find a suitable model	
	 
  	step 5 more data : the hidden factors:
  	 	1. There may be not only one model => different categories =>redesign the model
	 	2. Are there more hidden factors? => DOMAIN KNOWLEDGE (e.g. weight, height, HP=hit point)
		3. Include all the patameters
		=> low error on training data  BUT overfitting
		
	step 6 Regulation
		Back to step 2  =>  Σ real value - estimated value + λ(wi)<SUP>2</SUP>
				    λ(wi)<SUP>2</SUP> => the smaller the better for wi 
							 why: output has less influence(less sentitive) by noise input xi 
		Tiny higher error on training but lower error on testing data 
		=> The suitable model
Key Points:
	1.Model selection 
	2.Evaluation : loss function
	3.Gredient Decent
	4.Optimizing: another model selection
	5.Overfitting
	6.Regulation

Reference: https://www.youtube.com/watch?v=fegAeph9UaA
	   ML Lecture 1: Regression - Case Study

				    
		
		
	
